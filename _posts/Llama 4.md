# All About Llama 4

## 0. 개요

![1](/Users/jung0202/Library/Application Support/typora-user-images/1.png)

| 모델명   | 활성 파라미터 | 총 파라미터 | MOE  | 컨텍스트 윈도우 | 주요 특징                                 | 추론 하드웨어                |
| -------- | ------------- | ----------- | ---- | --------------- | ----------------------------------------- | ---------------------------- |
| Scout    | 17B           | 109B        | 16   | 10M             | 초장문 처리 + 비전 이해 최적화            | 단일 NVIDIA H100 (Int4)      |
| Maverick | 17B           | 400B        | 128  | 1M              | 고성능 멀티모달 범용 모델                 | 단일 NVIDIA H100 호스트      |
| Behemoth | 288B          | 2T          | 16   | 비공개          | 교사 모델, 초거대 및 초지능 멀티모달 모델 | 대규모 분산 학습 인프라 필요 |



## 1. 공통 특징

### - Mixture-of-Experts (MoE)

![2](/Users/jung0202/Library/Application Support/typora-user-images/2.png)

- 각 토큰은 공통 전문가(shared expert) + top-1 전문가 조합으로 라우팅
- 전문가별 로드 밸런싱(loss) 적용 → 편향 최소화

### - 멀티모달 Native Early Fusion

- 텍스트, 이미지, 비디오를 하나의 토큰 시퀀스로 변환해 동시 입력
- MetaCLIP 비전 인코더와 LLM을 공동 학습

### - 학습 효율 최적화 기술

- MetaP : layer별 학습률, 초기화 범위 등을 자동 설정하는 메타 최적화 알고리즘
- FP8 연산 : Hopper GPU 기반 e4m3 포맷 사용 → 효율 극대화 (GPU당 390 TFLOPs 이상)

### - 대규모·다국어 데이터 믹스

- 텍스트·이미지·비디오 합쳐 30조 토큰 이상, 200개 언어 커버 (100개 언어당 10억 토큰)

### - Pre-Mid-Post Training 흐름

1. Pre-training : 30조 토큰 이상 (텍스트 + 비전)
2. Mid-training : 256K 컨텍스트 학습, RoPE 제거 및 Interleaved Attention 기반
3. Post-training : SFT → RL → DPO로 구성된 다단계 미세조정



## 2. Llama 4 Scout : 초장문 멀티모달 이해 특화 모델

| 항목          | 세부 내용                                                    |
| ------------- | :----------------------------------------------------------- |
| 모델 아키텍처 | iRoPE 구조 <br />- Interleaved Attention (RoPE 없이 교차적 인코딩) <br />- Inference-time Temperature Scaling (장문 일반화 강화) |
| 멀티모달 처리 | 최대 8장 이미지 입력 지원, 비전 질의응답(VQA) 및 시각적 개념 정렬(image grounding) 최적화 |
| 컨텍스트 길이 | 최대 10M 토큰 지원 (현존 모델 중 최고 수준)                  |
| 기술적 특이점 | Needle-in-Haystack 벤치마크에서 Top-1 Recall 100% 기록       |
| 활용 예시     | 긴 문서 자동 요약, 대규모 코드베이스 분석, 멀티문서 기반 질의응답, 이미지 기반 복합 질문 응답 |

------

### - iRoPE 아키텍처

#### 1. Interleaved Attention

- 기존 : RoPE(Rotary Positional Embedding)를 통해 각 토큰의 상대 위치 인코딩

- RoPE 없이도 장문을 처리할 수 있는 새로운 Attention 구조 채택 : RoPE-Free Layer와 Regular Layer를 교차로 쌓아 구조화된 위치 정보를 간접적으로 학습

  → 10M 토큰 이상의 문맥에서도 문서 전반 일관되게 이해

#### 2. Inference-time Temperature Scaling

- 컨텍스트가 길어질수록 정보 희석(dilution) 문제가 발생하는데, temperature scaling으로 중요 토큰에 가중치 집중 가능

  → 긴 문서의 앞부분과 뒷부분 모두에 대해 정확하고 균형 잡힌 응답 생성 가능

![4](/Users/jung0202/Library/Application Support/typora-user-images/4.png)

### - 멀티이미지 기반 비전 최적화

#### 1. 학습 구조

- 학습 시 이미지/비디오 프레임 스틸을 최대 48장까지 활용하여 시각적 일관성과 정렬 능력을 훈련
- 최대 8장의 이미지를 동시에 입력받아 처리 가능

#### 2. 주요 기능 및 기술

| 기능명                             | 설명                                                         |
| ---------------------------------- | ------------------------------------------------------------ |
| 시각적 개념 정렬 (Image Grounding) | 자연어와 이미지 간의 의미 정렬 능력을 학습 → 예 : "왼쪽 상단의 파란 원"과 이미지 패치 위치를 정확히 매칭 |
| 영역 기반 질의응답 (Anchor QA)     | 이미지의 특정 영역(anchor box)를 기준으로 질문을 처리함 → 예 : "이 사람의 손에 들고 있는 물건은 무엇인가요?" |
| 다중 이미지 컨텍스트 통합          | 여러 장의 이미지에서 공통 시나리오 또는 시간 흐름을 추론할 수 있도록 학습함 → 예 : "앞 장면과 뒷 장면의 인물 변화 설명" |
| 시각-언어 융합 인코딩              | 이미지 패치를 텍스트 토큰 차원(예: 4096 차원)과 통일시켜 멀티모달 융합을 자연스럽게 수행 |

![image-20250425111449540](/Users/jung0202/Library/Application Support/typora-user-images/image-20250425111449540.png)

### - 성능

![5](/Users/jung0202/Library/Application Support/typora-user-images/5.png)



## 3. Llama 4 Maverick : 고성능 멀티모달 모델

| 항목      | 내용                                                         |
| --------- | ------------------------------------------------------------ |
| 아키텍처  | Dense ↔ MoE 교차 구조, 128 전문가 사용 (효율성 + 확장성 동시 확보) |
| 학습 전략 | Lightweight SFT → Online RL → Lightweight DPO                |
| 차별 요소 | Pass@k 기반 난이도 측정 + 쉬운 샘플 제거 + 적응형 RL 순환 구조 도입 |
| 활용 예시 | 범용 어시스턴트, 창의적 작문, 고난도 질의응답, 시각 인터페이스 지원 |

------

### - 전체 학습 파이프라인 요약

#### 1. Lightweight SFT

- 문제점 : 전통적인 SFT는 너무 많은 “쉬운” 예제를 포함 → 학습 초기에 편향 발생
- 해결 방법
  - Llama 계열 모델을 판정자(judge)로 활용하여 전체 SFT 데이터에서 쉬운 샘플을 50% 이상 제거
  - 남은 중간~고난이도(medium-to-hard) 샘플에 집중하여, 초기부터 복잡한 패턴을 학습
- 성과 : 모델이 단순한 지식 복사 대신 고난도 과제에 대한 표현력을 빠르게 확보

#### 2. Online RL (온라인 강화학습) + 적응형 난이도 필터링

- 핵심 전략 : 난이도가 높은 프롬프트를 중심으로 RL 수행
- Pass@k 기반 커리큘럼을 통해 생성 응답의 품질을 정량적으로 측정 → 난이도 평가에 반영
- 절차
  1. 모델 응답을 판정자에게 평가
  2. 고난도 샘플에 더 높은 보상(weight) 부여
  3. 모델 업데이트
  4. 업데이트된 모델로 전체 프롬프트 재평가 → 새로운 쉬운 샘플 제거
  5. 반복
- 효과
  - 컴퓨트 효율성 극대화: 쉬운 프롬프트 배제
  - 지속적 일반화 능력 향상: 점진적으로 더 어려운 문제에 모델이 적응

#### 3. Lightweight DPO

- 역할
  - RL 이후에도 남아 있는 논쟁적/모호한 케이스 튜닝
  - 대화 일관성 및 유연성 확보
- 방식
  - 소수의 고난도 프롬프트에 대해 사용자 선호 기반 응답 최적화
  - 모델의 최종 품질과 대화력 정밀 조율

### - 성능

![3](/Users/jung0202/Library/Application Support/typora-user-images/3.png)



## 4. Llama 4 Behemoth : 초거대, 초지능 멀티모달 모델

| 항목           | 내용                                                         |
| -------------- | ------------------------------------------------------------ |
| 모델 역할      | Llama 4 Maverick 및 Scout 모델의 지식 코디스틸링(교사) 모델  |
| 모델 규모      | 총 파라미터 2T, 활성 파라미터 288B (16 MoE 전문가 사용)      |
| 학습 전략      | 95% SFT 제거 + 고난이도 중심 커리큘럼 + 초대규모 온라인 RL   |
| 학습 인프라    | 완전 비동기 온라인 RL + 동적 GPU 자원 분산 프레임워크 구성   |
| 대표 성능 지표 | MATH-500, GPQA Diamond 등 고난이도 벤치마크에서 GPT-4.5 능가 |

------

### - Co-distillation : 고품질 전이 학습

- Maverick과 Scout의 사전학습 및 사후학습 전반에 걸쳐 교사 모델 역할 수행
- 새로운 손실 함수(loss function) 적용
  - Soft target (확률 분포)와 Hard target (정답 레이블) 간의 가중치를 동적으로 조절
  - 예 : 간단한 질문 → 정답 중심, 복잡한 질문 → 확률 분포 중심 학습

### - Post-training : 고난도 중심 학습 구성

- 기존 SFT 방식은 "쉬운 데이터 편향"으로 고급 문제에 약했음.
- 전체 SFT 데이터의 95%를 제거하고, 오직 복잡하고 다양한 고난이도 프롬프트만 활용
- 학습 설계 방식
  - Pass@k 기반 난이도 분석으로 쉬운/중간/어려운 데이터 계층화
  - Advantage=0 프롬프트 제거 : 답변 간 차이가 없거나 모호한 문제는 제외
  - 모달리티 다양성 기반 배치 구성 : 텍스트, 코드, 수학, 이미지가 혼합된 훈련 배치 설계

### - 최대 규모 강화학습 인프라

- 기존 RL 병목
  - 모든 전문가 모델을 메모리에 상주시켜 비효율적 계산
  - 연산 속도 불균형으로 GPU 리소스 낭비
- Behemoth의 혁신 구조
  - MoE 전문가 레벨 분산 최적화 : 활성 전문가만 GPU에 적재
  - 비동기 온라인 RL
    - 각 GPU는 독립적으로 데이터를 처리하고 RL 업데이트 수행
    - 성능 기반 자원 재할당으로 학습 효율 10배 이상 향상

### - 성능

![6](/Users/jung0202/Library/Application Support/typora-user-images/6.png)



## 5. 안전성 및 보안 기능

| 기능명       | 설명                                     |
| ------------ | ---------------------------------------- |
| Llama Guard  | 유해/위험 콘텐츠 출력 차단 모델          |
| Prompt Guard | 조작된 프롬프트 탐지 및 필터링           |
| CyberSecEval | LLM 사이버 보안 능력 평가                |
| GOAT         | 자동화된 적대 테스트 (Adversarial QA 등) |